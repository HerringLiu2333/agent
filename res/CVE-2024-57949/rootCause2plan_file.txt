1. Plan Summary
Detect functions used as irq_chip -> irq_set_vcpu_affinity callbacks that acquire spinlocks with IRQ-disabling variants that unconditionally re-enable interrupts on exit (e.g., guard(raw_spinlock_irq)), while the caller holds an IRQ-disabled irq_desc lock, causing premature interrupt re-enable and race windows.

2. Detection Steps
1) Step 1: Identify irq_set_vcpu_affinity callbacks — find struct irq_chip initializations and extract the function assigned to the irq_set_vcpu_affinity field (based on FILE_CONTENT where its_irq_chip.irq_set_vcpu_affinity = its_irq_set_vcpu_affinity).
   - Signals: struct initialization of irq_chip; field .irq_set_vcpu_affinity -> function symbol.
   - FP mitigation: Only consider functions referenced by irq_chip.irq_set_vcpu_affinity.

2) Step 2: Model caller IRQ-disabled context for these callbacks — assume irq_set_vcpu_affinity() core path holds irq_desc lock with raw_spin_lock_irqsave (from PATCH_DESCRIPTION and ROOTCAUSE_ANALYSIS).
   - Signals: Use the role “irq_chip.irq_set_vcpu_affinity callback” to infer IRQ-disabled calling context.
   - FP mitigation: Limit this assumption to irq_set_vcpu_affinity callbacks only (do not generalize to other callbacks without evidence).

3) Step 3: Within the callback function, detect usage of guard(raw_spinlock_irq) or equivalent IRQ-enabling unlock patterns.
   - Signals: Macro/function invocation matching guard(raw_spinlock_irq)(...), or lock/unlock pairs that unconditionally enable interrupts (e.g., raw_spin_lock_irq + raw_spin_unlock_irq) guarding critical sections.
   - FP mitigation: Exclude cases using non-IRQ variants (guard(raw_spinlock)) or save/restore variants that preserve state (irqsave/irqrestore semantics), as in the patched code (PATCH_DIFF).

4) Step 4: Flag if a guard(raw_spinlock_irq) is used to protect data structures in the callback.
   - Signals: Presence of guard(raw_spinlock_irq) within the callback scope, as in pre-patch its_irq_set_vcpu_affinity (PATCH_DIFF shows change to guard(raw_spinlock)).
   - FP mitigation: Ensure the guard protects a local critical section and that it exits normally (RAII or scope end); do not flag if immediately followed by logic that manually restores the IRQ-disabled state (not evidenced here).

5) Step 5: Identify other unconditional “*_unlock_irq” patterns inside the callback.
   - Signals: raw_spin_unlock_irq, spin_unlock_irq, or similar unconditional enabling calls that match a prior lock in the same function.
   - FP mitigation: Exclude lock/unlock pairs using irqsave/irqrestore (restore semantics), as those preserve the outer IRQ-disabled state.

6) Step 6: Correlate presence of unconditional IRQ-enabling lock usage with the inferred IRQ-disabled calling context to report a nested interrupt-state violation.
   - Signals: Callback role (Step 2) AND inner guard(raw_spinlock_irq) or unlock_irq usage (Steps 3–5).
   - FP mitigation: Require both conditions; do not report if callback lacks unconditional enable patterns.

7) Step 7: Highlight safer alternative usage if detected issue — recommend non-IRQ spinlock (guard(raw_spinlock)) for such callbacks.
   - Signals: Emit remediation hint when a violation is reported, aligning with the fix (PATCH_DIFF, ROOTCAUSE_ANALYSIS).
   - FP mitigation: Only suggest this when an IRQ-enabling variant is found in the callback.

8) Step 8: Optional deepening — within the same compilation unit, find any other function bound to irq_chip callbacks that might share the same calling context and perform the same lock pattern.
   - Signals: Scan other irq_chip callbacks in the same struct (if present) for guard(raw_spinlock_irq).
   - FP mitigation: Report as “suspect” with lower confidence unless there is evidence they are called under irq_desc irqsave locks (not provided here).

3. Target Elements
- Struct initializations of irq_chip and their field assignments.
- Function definitions referenced by irq_chip.irq_set_vcpu_affinity.
- Macro/function invocations that manipulate spinlocks and IRQ state:
  - guard(raw_spinlock_irq), raw_spin_lock_irq, raw_spin_unlock_irq, spin_lock_irq, spin_unlock_irq, and non-IRQ variants (for contrast).
- Intra-function control-flow scopes where guards apply.
- Call graph role inference (callback invoked by core irq_set_vcpu_affinity).

4. Dataflow / Taint Considerations
- Conceptually taint functions assigned to irq_chip.irq_set_vcpu_affinity as “IRQ-disabled caller context.”
- Propagate this property to the function body to assess inner lock usage that could unconditionally enable IRQs on exit.
- No detailed value-flow is required; this is context + API-pattern driven.

5. Validation & Test Cases
- Positive: A struct irq_chip with .irq_set_vcpu_affinity = f; and f contains guard(raw_spinlock_irq)(&lock); Expect a finding noting premature IRQ re-enable risk (mirrors pre-patch its_irq_set_vcpu_affinity per PATCH_DIFF and ROOTCAUSE_ANALYSIS).
- Negative: Same as above but f uses guard(raw_spinlock)(&lock); Expect no finding (matches patched code).
- Negative: A function unrelated to irq_chip.irq_set_vcpu_affinity uses guard(raw_spinlock_irq); Expect no finding (context not inferred as IRQ-disabled).
- Test harness notes: Ensure macro expansion is visible to the analyzer or model guard(...) as a recognizable call-like construct in the AST.

6. Estimated Effort & Priority
Medium — requires modeling struct field-role inference and recognition of IRQ-manipulating lock APIs; patterns are localized and well-scoped.

7. Likely False-Positive Sources & Mitigations
- Functions assigned to irq_set_vcpu_affinity but not actually called under irq_desc irqsave in non-standard kernels: mitigate by basing on ROOTCAUSE_ANALYSIS which states the kernel call chain behavior.
- Macros resolved differently across configurations: mitigate by matching both guard(raw_spinlock_irq) and equivalent raw_spin_*_irq pairs.
- Functions that re-disable IRQs after using unlock_irq: mitigate by checking for immediate IRQ-disabling actions post-guard (none shown in provided code).

8. Limitations & Assumptions
- Assumes irq_set_vcpu_affinity is called under irq_desc lock with raw_spin_lock_irqsave, as stated in PATCH_DESCRIPTION/ROOTCAUSE_ANALYSIS; the checker does not re-derive this from missing core code.
- Assumes guard(raw_spinlock_irq) unconditionally enables IRQs on scope exit (per ROOTCAUSE_ANALYSIS); macro semantics are not re-inferred from source here.
- Scope limited to irq_chip.irq_set_vcpu_affinity callbacks; other callbacks may have different calling contexts not covered by provided evidence.