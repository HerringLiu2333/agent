1. Plan Summary
Detect heap under-allocation where an array of non-byte elements (e.g., struct) is allocated with kmalloc/devm_kmalloc using only the element count (missing sizeof), and the related risk of reading uninitialized array elements that should be zeroed (prefer kcalloc), as exemplified by the wcn36xx chan_survey bug.

2. Detection Steps
1) Step 1: Objective — collect candidate allocation sites. Signals — calls to kmalloc/devm_kmalloc with a size argument; based on [PATCH_DIFF], devm_kmalloc was used pre-fix. FP mitigation — ignore other allocators unless they are known size-only allocators; focus on these two to stay precise.

2) Step 2: Objective — identify “count-only” size computations. Signals — size argument is a variable/expression not involving sizeof(...) or multiplication by a known element size; [ROOTCAUSE_ANALYSIS] cites using n_channels directly. FP mitigation — exclude sizes that already include sizeof or a compile-time constant matching element size.

3) Step 3: Objective — infer intended element type. Signals — the allocation result is assigned to/cast to a pointer to a non-byte type (e.g., struct T*, int*, or a typedef thereof); [PATCH_DIFF] shows struct wcn36xx_chan_survey as the element type. FP mitigation — exclude targets typed as char*/u8*/void* intended as raw byte buffers.

4) Step 4: Objective — confirm array semantics. Signals — subsequent uses like ptr[index], ptr[index].field, or deref after pointer arithmetic using an index indicate element-wise access; [ROOTCAUSE_ANALYSIS] mentions indexing per-channel survey data. FP mitigation — require at least one such indexed/field access after allocation in the same function or along a simple intra-procedural path.

5) Step 5: Objective — flag under-allocation. Signals — combine Steps 2–4: size equals count-only while ptr is used as an array of elements with element size > 1 byte; this maps directly to the undersized allocation root cause in [ROOTCAUSE_ANALYSIS]. FP mitigation — if static type of element is 1 byte (char/u8), do not flag.

6) Step 6: Objective — strengthen confidence via loop-bound correlation. Signals — detect that the same “count” expression used in the allocation is used as a loop bound or limit when indexing the array (e.g., for i < count then ptr[i]); [ROOTCAUSE_ANALYSIS] mentions using n_channels as the number of elements. FP mitigation — require a clear data equality or aliasing between size argument and loop bound variable.

7) Step 7: Objective — detect missing zero-initialization for array-of-structs at risk of read-before-write. Signals — allocation via kmalloc/devm_kmalloc (non-zeroing) and subsequent reads of struct fields on some path without a dominating write or explicit zeroing; [PATCH_DESCRIPTION] and [ROOTCAUSE_ANALYSIS] note uninitialized use risk. FP mitigation — conservatively require absence of memset/zeroing or field initialization dominating the read; ignore if analysis is inconclusive.

8) Step 8: Objective — identify safe patterns and suppress. Signals — allocation via kcalloc/kzalloc already ensures size correctness and zeroing; [PATCH_DIFF] switched to devm_kcalloc with count and sizeof. FP mitigation — do not flag any site using kcalloc/kzalloc or a kmalloc immediately followed by memset(..., 0, count * sizeof(T)) dominating all uses.

9) Step 9: Objective — account for managed allocations. Signals — treat devm_kmalloc/devm_kcalloc equivalently to kmalloc/kcalloc for sizing/initialization properties; [PATCH_DIFF] uses devm_kcalloc as the fix. FP mitigation — same as above; management aspect does not affect size or zeroing.

10) Step 10: Objective — report with context to reduce noise. Signals — include the inferred element type, size expression, and first indexed use location to justify the finding, tying it to the under-allocation and potential OOB described in [ROOTCAUSE_ANALYSIS]. FP mitigation — only report when both “count-only size” and “array-like use” are present.

3. Target Elements
- Call sites to kmalloc and devm_kmalloc.
- Lvalue assignments or casts receiving allocation results (to infer element type).
- Array indexing expressions and field accesses on pointer-to-struct arrays.
- Loops and conditional bounds that relate to the allocation count.
- Initialization patterns (memset, explicit field writes) and zeroing allocators (kcalloc/kzalloc).

4. Dataflow / Taint Considerations
- Track the allocation result variable/field through simple assignments and aliases to its array uses (indexing, field access).
- Track the “count” expression from the allocation site to loop bounds or conditional checks to correlate intended number of elements.
- For uninitialized-use detection, compute whether a write to the element or field dominates the read; treat memset/memset-like zeroing as initialization.

5. Validation & Test Cases
- Positive: struct foo *a = devm_kmalloc(dev, n, GFP_KERNEL); ... a[i].x = 1; or a[i].x read — expect under-allocation (and uninitialized-read if read occurs without prior writes).
- Positive: struct foo *a = kmalloc(n, GFP_KERNEL); for (i = 0; i < n; i++) use a[i] — expect under-allocation finding.
- Negative: struct foo *a = devm_kcalloc(dev, n, sizeof(*a), GFP_KERNEL); ... a[i] use — no findings.
- Negative: u8 *buf = kmalloc(n, GFP_KERNEL); for (i = 0; i < n; i++) buf[i] = 0; — no findings (byte buffer).
- Negative: struct foo *a = kmalloc(n * sizeof(*a), GFP_KERNEL); ... a[i] — no findings (size correct).
- Test harness notes: Build tests with type info available and ensure the analysis observes both allocation and subsequent array uses within the same function for straightforward validation.

6. Estimated Effort & Priority
High priority due to severity (heap overflow and uninitialized use) per [ROOTCAUSE_ANALYSIS]; medium implementation effort leveraging existing type and dataflow capabilities.

7. Likely False-Positive Sources & Mitigations
- Byte-oriented buffers allocated by count only; mitigate by excluding char/u8/void pointers.
- Allocations intended for raw byte blobs but later cast to struct for a single element; mitigate by requiring array-like indexed use.
- Complex wrapper allocators/macros not recognized; mitigate by scoping to kmalloc/devm_kmalloc initially.
- Incomplete dominance analysis for initialization; mitigate by only flagging uninitialized reads when clear lack of dominating writes is shown.

8. Limitations & Assumptions
- Assumes type information is available to distinguish struct/element sizes; otherwise, element-size > 1 inference may be imprecise.
- Does not cover custom allocation wrappers beyond kmalloc/devm_kmalloc mentioned in [PATCH_DIFF] and [ROOTCAUSE_ANALYSIS].
- Uninitialized-read detection is conservative and may miss cases where initialization occurs via indirect calls or inter-procedurally.
- The plan generalizes beyond the specific variable n_channels, but cannot rely on naming conventions; it infers intent from types and uses.