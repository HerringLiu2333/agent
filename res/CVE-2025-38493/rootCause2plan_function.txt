1. Plan Summary
Detect memcpy/memmove writes into struct fields that are annotated as counted_by another field, when the counted-by length field is not initialized before the copy (incorrect initialization order), as in the pre-patch __timerlat_dump_stack pattern.

2. Detection Steps
1) Step 1: Objective — Identify memory-copy calls relevant to the issue. Signals — Calls to memcpy/memmove where the destination is a struct field or address-of a struct field (e.g., &entry->caller). FP mitigation — Restrict to destinations that are fields of a struct, not arbitrary pointers.

2) Step 2: Objective — Recognize counted-by-managed destinations. Signals — The destination field is annotated with a counted_by attribute referring to a sibling length field (e.g., caller is __counted_by(size), per [PATCH_DESCRIPTION]). FP mitigation — Require a resolvable annotation linking the destination field to a specific length field in the same struct.

3) Step 3: Objective — Bind the concrete object instance being written. Signals — The destination expression’s base object (e.g., entry from ring_buffer_event_data(event) in [FUNCTION_CONTENT]) and the specific length field it refers to (e.g., entry->size). FP mitigation — Include basic alias resolution (same variable/field across assignments) and ignore writes through unrelated aliases.

4) Step 4: Objective — Determine write order to length vs. data fields. Signals — Control-flow ordering where the first write to the destination field (memcpy/memmove into counted_by field) occurs before any assignment to the counted length field (e.g., memcpy(&entry->caller, ...) precedes entry->size = ... as shown pre-patch in [FUNCTION_CONTENT]/[PATCH_DIFF]). FP mitigation — Require that there exists a subsequent assignment to the length field in the same function/path (confirming the “late init” pattern rather than “never set”).

5) Step 5: Objective — Confirm the length field is read by the bounds logic at copy time. Signals — The destination is the counted_by field, so FORTIFY checks will use the length field’s current value (based on [PATCH_DESCRIPTION] and [ROOTCAUSE_ANALYSIS]). FP mitigation — Exclude cases where the counted_by field is not present or the destination is not the specific counted_by child field.

6) Step 6: Objective — Prioritize cases where the object storage is plausibly uninitialized at copy time. Signals — The base object is obtained from functions analogous to ring_buffer_event_data() after a reserve (trace_buffer_lock_reserve), as in [FUNCTION_CONTENT]/[ROOTCAUSE_ANALYSIS]; treat these as sources of uninitialized struct contents before explicit writes. FP mitigation — Add a positive signal if the object originates from ring_buffer_event_data or similar “buffer/event/trace” acquisition functions mentioned in [FUNCTION_CONTENT]; if such a signal is absent, require even stronger evidence from Steps 2–4.

7) Step 7: Objective — Detect unsafe third-argument usage that suggests a non-zero copy. Signals — The memcpy length argument is a variable or expression not provably zero (e.g., “size” parameter in [FUNCTION_CONTENT]); this matches the panic “88 byte write of buffer size 0” in [PATCH_DESCRIPTION]. FP mitigation — Ignore calls where the copy length is provably zero.

8) Step 8: Objective — Filter out correct initialization order. Signals — Do not flag if there is any dominating assignment to the counted length field (entry->size = …) strictly before the memcpy into the counted_by field along all paths reaching the call (as in the fixed order in [PATCH_DIFF]). FP mitigation — Use dominance/path sensitivity to ensure the length field is definitely set before the copy.

9) Step 9: Objective — Handle alternative memory write APIs. Signals — Apply the same analysis to memmove and other bulk copy helpers, if present, that write into the counted_by field. FP mitigation — Only include standard C bulk-copy APIs to avoid overbroad pattern matching.

10) Step 10: Objective — Highlight matches analogous to the CVE scenario. Signals — Report when: destination is a counted_by-managed field; copy precedes any assignment to the counted length; and a later assignment to that same length field exists in the function (matching the “swap order” fix in [PATCH_DIFF]). FP mitigation — Require all three signals to minimize false positives.

3. Target Elements
- Function bodies containing memcpy/memmove calls.
- Call sites where destination is a struct field or address-of a struct field.
- Struct field metadata/attributes linking a field to a counted_by length field.
- Assignments to the counted length field (e.g., entry->size = ...).
- Sources returning newly obtained event/buffer objects (e.g., ring_buffer_event_data, trace_buffer_lock_reserve usage context).

4. Dataflow / Taint Considerations
- Track the base struct object from its acquisition (e.g., ring_buffer_event_data(event)) to uses, following simple aliases.
- Track control-flow order of writes: whether entry->size is assigned before the memcpy into entry->caller on all paths.
- Track the memcpy length argument to ensure it is not trivially zero.

5. Validation & Test Cases
- Positive: Pre-patch __timerlat_dump_stack in [FUNCTION_CONTENT], where memcpy to entry->caller occurs before entry->size assignment; expect a finding.
- Positive: A variant using memmove into a counted_by field with the later length assignment; expect a finding.
- Negative: Post-patch order in [PATCH_DIFF] where entry->size is set before memcpy; expect no finding.
- Negative: memcpy into a field without any counted_by annotation; expect no finding.
- Test harness notes: Include a model of __counted_by(size) on the destination field and the presence of ring_buffer_event_data to demonstrate prioritization of uninitialized storage.

6. Estimated Effort & Priority
High: requires modeling counted_by relationships, control-flow ordering, and basic alias/path sensitivity; high security impact per [ROOTCAUSE_ANALYSIS].

7. Likely False-Positive Sources & Mitigations
- Missing counted_by metadata causing heuristic-only matches: mitigate by requiring explicit counted_by linkage where available.
- Cases where the struct is fully initialized elsewhere before the copy via helper inlined paths: mitigate with path sensitivity and dominance checks local to the function.
- Zero-length or guarded copies: mitigate by checking the copy size is not provably zero.

8. Limitations & Assumptions
- Assumes the analysis can resolve counted_by annotations linking data and length fields; this is inferred from [PATCH_DESCRIPTION], but the struct definition is not provided here.
- Assumes ring_buffer_event_data provides uninitialized storage as per [ROOTCAUSE_ANALYSIS]; the checker prioritizes but does not mandate this signal.
- Does not verify actual buffer capacity or exact FORTIFY behavior; it flags based on incorrect initialization order relative to counted_by semantics.