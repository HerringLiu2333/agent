1. Plan Summary
Detect order-of-initialization bugs where code copies into a __counted_by-managed array field of an event buffer struct before initializing the corresponding size field, as in the pre-patch __timerlat_dump_stack() sequence that crashed under FORTIFY checks.

2. Detection Steps
1) Step 1: Objective: Find memcpy/memmove-like writes into counted arrays. Signals: Call sites of memory-copy functions where the destination is an address of a struct field that is annotated with a counted-by size field or known to be stack_entry->caller (based on PATCH_DESCRIPTION/ROOTCAUSE_ANALYSIS). FP mitigation: Prefer attribute-based recognition of __counted_by(size); if unavailable, limit to the known pattern of struct stack_entry->caller.

2) Step 2: Objective: Identify uninitialized event-payload objects. Signals: In the same function, an lvalue pointer (e.g., entry) assigned from ring_buffer_event_data(event) after a prior trace_buffer_lock_reserve call (see FILE_CONTENT and PATCH_DIFF). FP mitigation: Require that the destination object of the memcpy/memmove is exactly the one returned by ring_buffer_event_data in the current function.

3) Step 3: Objective: Establish ordering since allocation. Signals: A control-flow path from the assignment entry = ring_buffer_event_data(event) to the memcpy/memmove into entry->caller without any intervening assignment to entry->size. FP mitigation: Path-sensitive check within the function; ignore separate functions or aliasing that cannot be resolved.

4) Step 4: Objective: Confirm “eventual” size initialization (indicating misordering rather than missing initialization). Signals: Detect a subsequent assignment to entry->size (e.g., entry->size = …) later on the same path after the memcpy/memmove (as in PATCH_DIFF reordering). FP mitigation: Only flag when such a later write exists, reducing noise from incomplete construction code that may set size in a different builder.

5) Step 5: Objective: Ensure the copy length is independent from the counted size (heightened risk). Signals: The third argument to memcpy/memmove is an expression not derived from entry->size (e.g., a local variable like size computed from fstack->stack_size per FILE_CONTENT). FP mitigation: If the length equals entry->size or is bounded by it before the copy, de-prioritize or suppress the alert.

6) Step 6: Objective: Confirm the payload buffer starts uninitialized, making the counted_by check use garbage. Signals: Absence of any prior write to entry->size on all paths from ring_buffer_event_data to the memcpy; and no full-struct initialization that sets size correctly before the memcpy. FP mitigation: Do not treat memset-zero as safe (per ROOTCAUSE_ANALYSIS, zero breaks FORTIFY), but suppress if there’s a definite earlier write of the correct size field.

7) Step 7: Objective: Recognize trace event construction contexts where this pattern is common. Signals: The presence of trace_buffer_lock_reserve/trace_buffer_unlock_commit_nostack bracketing, and copying into a field of the event payload between them (as in FILE_CONTENT). FP mitigation: Restrict initial version of the rule to this trace-event construction pattern to lower FPs.

8) Step 8: Objective: Identify multiple counted arrays if present. Signals: For any struct with counted_by-annotated arrays, repeat Steps 1–7 per array/size pair in the same function. FP mitigation: Only flag pairs where the specific size field associated to that array is assigned after the copy.

9) Step 9: Objective: Exclude safe reorders. Signals: If an assignment to entry->size is guaranteed (dominator analysis) before the memcpy call along all paths since entry assignment, do not flag. FP mitigation: Require dominance or proven path coverage for the size assignment.

10) Step 10: Objective: Prioritize high-confidence instances. Signals: Exact match to the pre-patch pattern: memcpy(&entry->caller, …) followed later by entry->size = … in a function that obtains entry via ring_buffer_event_data (per FILE_CONTENT/PATCH_DIFF). FP mitigation: Mark these as highest severity (crash potential per PATCH_DESCRIPTION/ROOTCAUSE_ANALYSIS).

3. Target Elements
- Calls to ring_buffer_event_data and the variable receiving its result.
- Calls to trace_buffer_lock_reserve/trace_buffer_unlock_commit_nostack.
- Calls to memcpy/memmove writing into struct fields.
- Field assignments to size fields (e.g., entry->size).
- Struct fields annotated with __counted_by(size) or the known stack_entry->caller field.
- Intra-procedural control-flow and dominance relationships within the function.

4. Dataflow / Taint Considerations
- Track the dataflow from ring_buffer_event_data(event) to the pointer variable (entry) and its field accesses.
- Track writes to entry->size along paths from allocation to memcpy, and from memcpy to function exit.
- Track the memcpy length expression to check independence from entry->size.
- Maintain path-sensitive ordering (from allocation to memcpy to size assignment).

5. Validation & Test Cases
- Positive: Pre-patch __timerlat_dump_stack pattern (PATCH_DIFF before): memcpy into &entry->caller occurs before entry->size assignment after entry = ring_buffer_event_data(event); expect a report.
- Negative: Post-patch __timerlat_dump_stack (PATCH_DIFF after): entry->size set before memcpy; expect no report.
- Positive: A synthetic function constructing a TRACE_STACK-like event where a counted_by array is copied before size is set, with size set later; expect a report.
- Negative: A function that sets entry->size immediately after ring_buffer_event_data and only then performs memcpy; expect no report.
- Test harness notes: Build both CONFIG_TIMERLAT_TRACER + CONFIG_STACKTRACE guarded paths and generic paths to ensure presence of ring_buffer_event_data context.

6. Estimated Effort & Priority
Medium. The rule requires attribute recognition, intra-procedural path ordering, and modest dataflow; impact is high (kernel panic) per ROOTCAUSE_ANALYSIS, so priority is high.

7. Likely False-Positive Sources & Mitigations
- Missing visibility of __counted_by annotations: Mitigate by targeting known stack_entry->caller pattern and trace-event contexts.
- Indirect or helper-based assignments to size fields not recognized intra-procedurally: Mitigate by restricting to explicit assignments in the same function or using conservative dominance.
- Complex aliasing of entry or casting obscuring field access: Mitigate by requiring straightforward field dereferences on the same variable bound to ring_buffer_event_data.

8. Limitations & Assumptions
- Assumes the analysis can identify __counted_by(size) annotations or at least the stack_entry->caller/size pattern (per PATCH_DESCRIPTION/ROOTCAUSE_ANALYSIS); the struct definition is not in FILE_CONTENT.
- Assumes ring_buffer_event_data returns uninitialized payload (stated in ROOTCAUSE_ANALYSIS); the checker relies on this to justify the risk.
- The plan is intra-procedural; patterns where size is set in a separate helper before memcpy may be missed.