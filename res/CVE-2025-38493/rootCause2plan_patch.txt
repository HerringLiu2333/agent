1. Plan Summary
Detect memory writes into struct fields annotated with __counted_by(count_field) where the corresponding count_field on the specific instance is not initialized (or is initialized only after) before the write, mirroring the pre-patch flaw in __timerlat_dump_stack().

2. Detection Steps
1) Step 1: Objective — Identify counted-by relationships; Signals — Struct fields declared with a counted_by or __counted_by annotation naming a sibling count field; Why — Based on ROOTCAUSE_ANALYSIS and PATCH_DESCRIPTION noting struct stack_entry’s caller is __counted_by(size); FP mitigation — Restrict to recognized counted_by-style macros/attributes in Linux code (e.g., __counted_by(name)).

2) Step 2: Objective — Find writes into counted-by fields; Signals — Calls to raw memory write APIs (e.g., memcpy/memmove) whose destination is the address of a counted-by field (e.g., &obj->caller); Why — The crash arose from memcpy into entry->caller (PATCH_DIFF, ROOTCAUSE_ANALYSIS); FP mitigation — Focus on non-zero or variable length copies (third argument not a constant zero), and exclude obvious constant-size copies smaller than or equal to a single element unless the field is an array.

3) Step 3: Objective — Bind the specific object instance and its count field; Signals — From the destination expression, extract the base object (e.g., entry) and the named count field from the annotation (e.g., size); Why — FORTIFY checks use the count field of the same instance (ROOTCAUSE_ANALYSIS); FP mitigation — Ensure the count field exists in the same struct and is field-selected from the same base object.

4) Step 4: Objective — Check initialization order within the function; Signals — Control-flow/AST dominance: determine if an assignment to base.count_field (e.g., entry->size = …) dominates the memcpy call along all paths; Why — Root cause is “memcpy before initializing size” (PATCH_DIFF shows reordering); FP mitigation — Require the assignment to dominate the call; if it post-dominates or is absent, mark suspicious.

5) Step 5: Objective — Prioritize cases where the object is newly obtained and likely uninitialized; Signals — The base object is produced by known allocation/factory functions or buffer accessors, e.g., ring_buffer_event_data(...) as in PATCH_DIFF; Why — ROOTCAUSE_ANALYSIS states entry contains garbage from the ringbuffer prior to field initialization; FP mitigation — Elevate severity/confidence when the base object originates from such APIs.

6) Step 6: Objective — Account for potential earlier initialization in the same function; Signals — Any prior write to base.count_field that dominates the memcpy, including through aliases to the same base; Why — Ensures we don’t flag when the count was set earlier, preventing the FORTIFY mismatch; FP mitigation — Use alias-aware matching for writes to the same instance (e.g., via temporary pointers to entry).

7) Step 7: Objective — Exclude cases where the copy length is provably compatible with the count or where the counted-by field is not used; Signals — If there is an immediate assignment of base.count_field to a value that equals or bounds the copy length and dominates the memcpy, consider safe; Why — This mirrors the fixed pattern “entry->size = fstack->nr_entries; memcpy(...)” (PATCH_DIFF); FP mitigation — Require a clear dominance and same instance binding.

8) Step 8: Objective — Detect inverted order patterns explicitly; Signals — A write to base.count_field occurs only after the memcpy in the same basic block or post-dominating path; Why — This exactly replicates the buggy order (ROOTCAUSE_ANALYSIS); FP mitigation — Only flag when no other dominating initialization exists.

9) Step 9: Objective — Deprioritize benign short writes that cannot exceed a minimal in-bounds array size; Signals — If the counted-by field governs an array of fixed known minimal capacity of 1 and the copy length is a small constant that fits, deprioritize; Why — Reduces noise when overflow cannot happen despite delayed count set; FP mitigation — Apply only when type/array element size and copy size make this provable and the destination field is truly fixed-capacity.

10) Step 10: Objective — Highlight patterns analogous to the reported subsystem; Signals — Functions under tracing/osnoise or similar stack-dump paths writing to ring-buffer entries before setting size; Why — ROOTCAUSE_ANALYSIS ties the issue to tracing/osnoise stack saving and dumping; FP mitigation — Mark these as high confidence due to exact match with the scenario.

3. Target Elements
- Struct field declarations annotated with counted_by/__counted_by.
- Calls to memory copy/move functions writing into those fields.
- Assignments to the corresponding count fields on specific object instances.
- Function-level control-flow/dominance relationships around the copy and the count assignment.
- Object origins from allocator/factory APIs (e.g., ring_buffer_event_data).

4. Dataflow / Taint Considerations
- Track the base object instance from the memcpy destination to its count field writes, including aliases and temporary pointers.
- Track origins of the base object from known allocation/factory functions to boost confidence that prior contents are uninitialized (e.g., ring_buffer_event_data as seen in PATCH_DIFF).
- Establish dominance: count field assignment must flow along all paths to the copy site; otherwise, report.

5. Validation & Test Cases
- Positive: A function that gets entry = ring_buffer_event_data(...); then memcpy(&entry->caller, src, size); then entry->size = n; Expected: flagged due to count set after copy.
- Negative: A function that sets entry->size = n; then memcpy(&entry->caller, src, size); Expected: not flagged.
- Negative: A function with an earlier dominating entry->size = n; then conditional paths leading to memcpy; Expected: not flagged if dominance holds.
- Positive: A function that never assigns entry->size before memcpy and assigns it only in a later block; Expected: flagged.
- Test harness notes: Run on a reduced kernel snippet mirroring __timerlat_dump_stack() pre- and post-patch to verify order sensitivity.

6. Estimated Effort & Priority
High.

7. Likely False-Positive Sources & Mitigations
- Count field initialized in a different function or earlier in lifetime: mitigate by requiring intra-procedural dominance or by raising severity only when object comes from known uninitialized sources.
- Macros obscuring assignments to the count field: mitigate by expanding macro calls where possible and matching field writes via aliases.
- Cases where counted_by is not recognized due to macro indirection: mitigate by matching common macro names used in Linux (e.g., __counted_by).

8. Limitations & Assumptions
- Assumes the analysis can identify counted_by annotations; macro resolution may be required.
- Does not prove runtime size relationships; focuses on initialization-before-use ordering per ROOTCAUSE_ANALYSIS and PATCH_DIFF.
- Inter-procedural initialization of the count field is not fully modeled; the plan emphasizes intra-procedural dominance to limit false positives.
- Specific allocator identification is limited to examples mentioned (e.g., ring_buffer_event_data) per PATCH_DIFF; broader allocator sets are not enumerated here.