1. Plan Summary
Detect out-of-bounds source reads caused by using a rounded-up (padded/aligned) size as the copy length when copying from a pre-existing source into a newly allocated padded buffer.

2. Detection Steps
1) Step 1: Identify candidate memory copy sites. Look for calls to byte-wise copy APIs (e.g., memcpy/memmove) whose length argument is a variable that was reassigned earlier. This focuses the analysis on places where copy size is computed, as seen in the pre-patch memcpy(tmp, info, info_len) case in [PATCH_DIFF].

2) Step 2: Detect rounding/alignment inflation of the length variable. Locate assignments where the same length variable is updated via a rounding/alignment function call (e.g., round_up) or an equivalent arithmetic pattern, especially following a modulo check (len % C). This maps directly to the root cause “length rounded up for padding” in [ROOTCAUSE_ANALYSIS] and evidenced by info_len = round_up(info_len, 4) in [PATCH_DIFF]; require that the update post-dominates the modulo condition to reduce false positives.

3) Step 3: Verify destination allocation uses the rounded size. Ensure a heap allocation (e.g., kunit_kzalloc/kmalloc/kzalloc) is performed with the rounded length variable and the allocation result is the destination of the copy. This ties the rounded value to destination padding intention (per [ROOTCAUSE_ANALYSIS] and [PATCH_DIFF]) and filters cases where rounding wasn’t for allocation.

4) Step 4: Confirm source is a pre-existing buffer. Require that the copy source is a different pointer from the just-allocated destination and is not newly allocated in the same path. This reflects the flaw “reading from original source buffer with padded size” (per [ROOTCAUSE_ANALYSIS]) and avoids self-copy or destination-initialization scenarios.

5) Step 5: Ascertain that the rounded length flows into the copy length. Track dataflow from the rounding assignment to the third argument of the memcpy/memmove call. This directly captures the conflation of destination padded size with copy length described in [ROOTCAUSE_ANALYSIS].

6) Step 6: Check that the original (pre-rounded) length is not used for copying. Verify there is no separate variable capturing the pre-rounded size that is then used as the copy length (e.g., copy_len = info_len; memcpy(..., copy_len)), as in the fix in [PATCH_DIFF]. If such a preserved length is used for copy, suppress the alert (negative signal).

7) Step 7: Strengthen proof of length increase along the path. Prefer paths guarded by a condition like len % C != 0 followed by rounding, which guarantees the new size is larger than the old (as in [PATCH_DIFF]). This reduces false positives where round_up might be a no-op.

8) Step 8: Ensure destination is sized to the rounded value. Confirm the allocation size equals or dominates the rounded length variable and that the copy’s destination is that allocation. This reflects the intended padded destination (per [ROOTCAUSE_ANALYSIS]) and prevents misattribution when the rounded value is unrelated to allocation.

9) Step 9: Exclude bounded-copy patterns. Suppress when the copy length is min(original_len, rounded_len) or otherwise proven not to exceed the pre-rounded length (e.g., via an explicit comparison). This addresses scenarios where developers already guard the copy length, reducing false positives.

10) Step 10: Prefer zero-initialized padded allocations. Boost confidence when the allocation is zero-initialized (e.g., kunit_kzalloc/kzalloc) or followed by memset to zero, indicating intent to pad with zeros and copy only meaningful bytes. This mirrors the fixed approach where the padded tail remains zero (per [PATCH_DIFF] and [ROOTCAUSE_ANALYSIS]).

11) Step 11: Report on the copy site with context. Emit findings at the memcpy/memmove call, referencing the earlier rounding and allocation statements as evidence of the padding/copy-length conflation. This maps clearly to the OOB read mechanism identified in [ROOTCAUSE_ANALYSIS].

12) Step 12: Prioritize matches with the exact round_up pattern. Assign highest confidence when the function name round_up is used (as in [PATCH_DIFF]); treat generic alignment arithmetic as lower confidence. This focuses on the concrete pattern observed in the vulnerability.

3. Target Elements
- Function bodies containing: length variable computations, condition checks (modulo), assignments.
- Calls to rounding/alignment helpers (e.g., round_up).
- Allocation sites (kunit_kzalloc, kzalloc, kmalloc) and their assigned targets.
- Memory copy call sites (memcpy, memmove) and their arguments.
- Dataflow between variables representing lengths and pointers (source/destination).
- Dominance/branching conditions around modulo checks and rounding.

4. Dataflow / Taint Considerations
- Track the “length” variable through: initial value -> modulo check -> rounding assignment -> allocation size -> memcpy length.
- Track the “destination” pointer from allocation to memcpy destination, ensuring identity.
- Track the “source” pointer to ensure it is distinct from the new allocation and not reallocated in-path.
- Detect presence/absence of a preserved pre-rounded length flowing into the copy length (fixed pattern in [PATCH_DIFF]).

5. Validation & Test Cases
- Positive: Code that does if (len % 4) { len = round_up(len, 4); tmp = kzalloc(len); memcpy(tmp, src, len); } where src points to pre-existing data. Expect a report at memcpy for potential OOB read.
- Negative: The fixed pattern from [PATCH_DIFF]: copy_len = len; len = round_up(len, 4); tmp = kzalloc(len); memcpy(tmp, src, copy_len); Expect no report.
- Negative: Both src and dst are allocated with the same rounded length in the same path before memcpy. Expect no report.
- Test harness notes: Run flow-sensitive tests to ensure dominance analysis (condition -> rounding -> allocation -> copy) is respected, and that variable reassignments are correctly tracked.

6. Estimated Effort & Priority
Medium: Requires inter-procedural dataflow for variables and control-flow reasoning, but focuses on a narrow, well-defined pattern (round_up + alloc + memcpy) as shown in [PATCH_DIFF] and [ROOTCAUSE_ANALYSIS].

7. Likely False-Positive Sources & Mitigations
- Cases where the source buffer is guaranteed to be at least as large as the rounded size but not provable statically; mitigate by requiring the explicit modulo-triggered rounding path and allocation with the same rounded size.
- Generic alignment arithmetic not using round_up; mitigate by lowering confidence or requiring stronger evidence (modulo check + alloc linkage).
- Copies guarded by preceding bounds checks not recognized; mitigate by detecting min-based or comparison-guarded copy lengths and suppressing.

8. Limitations & Assumptions
- Assumes availability of semantic knowledge for round_up and allocation/copy APIs; only round_up is explicitly evidenced in [PATCH_DIFF].
- Cannot always prove the actual size of the source buffer; relies on control/dataflow cues (modulo-triggered rounding) from [ROOTCAUSE_ANALYSIS].
- Focuses on patterns where destination allocation and memcpy occur in the same function/path; inter-procedural allocations for src sizing may be missed.