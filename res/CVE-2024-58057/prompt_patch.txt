1. CVE Identifier
CVE-2024-58057

2. Vulnerability Type
Denial of Service via CPU-bound workqueue starvation (scheduling/affinity misuse)

3. Root Cause Summary
The IDPF driver created multiple workqueues with flags=0 (per-CPU bound) in idpf_probe(), causing work items to execute only on the kworker tied to the CPU they were enqueued on. Under load, a misconfigured or high-priority process could monopolize that CPU’s time, starving the kworker and delaying driver work processing. The absence of WQ_UNBOUND (and WQ_MEM_RECLAIM) prevented migration to another CPU and guaranteed worker availability, leading to prolonged delays, timeouts, and an eventual system crash. The patch corrects this by converting the workqueues to unbound and reclaim-capable, allowing execution on any CPU within the NUMA node and ensuring forward progress.

4. Kernel Subsystem Analysis
1) Affected Subsystem:
Intel IDPF network driver (drivers/net/ethernet/intel/idpf), initialization path idpf_probe()

2) Pre-Patch Flaw:
Workqueues init_wq, serv_wq, mbx_wq, stats_wq, and vc_event_wq were created with alloc_workqueue using flags=0 (per-CPU bound), e.g., “alloc_workqueue(..., 0, 0, ...)”. This bound the driver’s asynchronous work to specific CPU kworkers with no ability to migrate when that CPU became overloaded. No use of WQ_UNBOUND or WQ_MEM_RECLAIM was present.

3) Trigger Condition:
When driver work (e.g., via queue_delayed_work/mod_delayed_work) is enqueued to run on a CPU that is heavily loaded or hogged (e.g., a high-priority task pinned to CPU0), the per-CPU kworker receives less than 0.5% CPU time. This causes large delays between workqueue_queue_work and workqueue_execute_start (observed up to ~30 ms).

4) Impact Mechanism:
Starvation of the per-CPU kworker delays execution of critical driver tasks, leading to performance degradation, timeouts in the driver’s workflows, and eventual system crash. In effect, a local process can induce a denial-of-service by monopolizing the CPU servicing the driver’s bound workqueues.

5. Patch Analysis
1) Fix Approach:
Convert all affected workqueues to WQ_UNBOUND | WQ_MEM_RECLAIM so that work can execute on any CPU within the NUMA node, avoiding single-CPU contention, and ensuring worker availability under memory pressure. This removes the dependency on a specific CPU’s kworker and mitigates starvation.

2) Key Code Changes:
In idpf_probe():
- init_wq: alloc_workqueue(..., WQ_UNBOUND | WQ_MEM_RECLAIM, 0, ...)
- serv_wq: alloc_workqueue(..., WQ_UNBOUND | WQ_MEM_RECLAIM, 0, ...)
- mbx_wq: alloc_workqueue(..., WQ_UNBOUND | WQ_MEM_RECLAIM, 0, ...)
- stats_wq: alloc_workqueue(..., WQ_UNBOUND | WQ_MEM_RECLAIM, 0, ...)
- vc_event_wq: alloc_workqueue(..., WQ_UNBOUND | WQ_MEM_RECLAIM, 0, ...)
These replace prior “alloc_workqueue(..., 0, 0, ...)” calls.

3) Locking/Concurrency Impact:
No explicit locking changes; the patch alters scheduling semantics by using unbound worker-pools. Work items can migrate across CPUs within the NUMA node, reducing single-CPU contention and improving forward progress. WQ_MEM_RECLAIM provides reserved workers to avoid stalls during memory reclaim. Overall concurrency is improved by decoupling from per-CPU kworker starvation.

6. Broader Kernel Security Implications
Per-CPU bound workqueues in drivers can be a denial-of-service vector: a local process can starve the servicing kworker by monopolizing its CPU. Using WQ_UNBOUND reduces susceptibility to such single-CPU starvation and improves resilience against noisy neighbors and misconfiguration. Adding WQ_MEM_RECLAIM further decreases the risk of stalls under memory pressure, improving reliability of critical driver paths. This patch highlights the need to review workqueue flags in performance-sensitive or critical subsystems to prevent local DoS via scheduler affinity and resource contention.