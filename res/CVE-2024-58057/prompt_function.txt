1. CVE Identifier
CVE-2024-58057

2. Vulnerability Type
Denial of Service (DoS) via workqueue starvation caused by CPU-bound worker affinity

3. Root Cause Summary
The IDPF driver created several critical workqueues with default (bound) worker-pool semantics, using alloc_workqueue flags “0, 0” in idpf_probe(). Bound workqueues tie execution to the CPU on which the work was enqueued, making them vulnerable to starvation when that CPU is heavily loaded or pinned by a high-priority task. This led to excessive delays in executing driver work items (service, mailbox, statistics, virtchnl events), causing timeouts, performance degradation, and eventual system crash. The patch corrects this by creating these workqueues as WQ_UNBOUND | WQ_MEM_RECLAIM, allowing migration to other CPUs in the same NUMA node and ensuring forward progress under pressure.

4. Kernel Subsystem Analysis
1) Affected Subsystem:
Networking driver (Intel IDPF), drivers/net/ethernet/intel/idpf/idpf_main.c

2) Pre-Patch Flaw:
idpf_probe() initialized adapter->init_wq, serv_wq, mbx_wq, stats_wq, and vc_event_wq with alloc_workqueue(..., 0, 0, ...), producing bound workqueues backed by per-CPU kworkers. These queues handled delayed tasks such as idpf_service_task, idpf_mbx_task, idpf_statistics_task, and idpf_vc_event_task via queue_delayed_work() without specifying a CPU, thereby inheriting the enqueue CPU for execution.

3) Trigger Condition:
A misconfigured or malicious high-priority, CPU-affined userspace process pinned to a specific CPU (e.g., CPU0) hogs CPU time, starving the bound kworker on that CPU. Because the work items are bound to the starved CPU (default configuration), their execution is arbitrarily delayed.

4) Impact Mechanism:
Driver work items accumulate and time out, disrupting control-plane operations (mailbox/virtchnl events), service routines, and stats handling; delays observed up to 30 ms. The cascading timeouts lead to performance degradation and can culminate in a system crash, representing a practical DoS vector against the system’s networking stack and stability.

5. Patch Analysis
1) Fix Approach:
Convert the driver’s workqueues from bound to unbound and enable MEM_RECLAIM. Using WQ_UNBOUND allows work items to execute on any CPU within the same NUMA node, mitigating starvation on a single CPU. WQ_MEM_RECLAIM ensures a rescuer thread for forward progress under resource pressure.

2) Key Code Changes:
In idpf_probe(), all alloc_workqueue() calls are changed from flags “0, 0” to “WQ_UNBOUND | WQ_MEM_RECLAIM, 0” for:
- adapter->init_wq (“%s-%s-init”)
- adapter->serv_wq (“%s-%s-service”)
- adapter->mbx_wq (“%s-%s-mbx”)
- adapter->stats_wq (“%s-%s-stats”)
- adapter->vc_event_wq (“%s-%s-vc_event”)

3) Locking/Concurrency Impact:
No explicit lock changes were made. Scheduling semantics shift from per-CPU bound workers to unbound pools, increasing execution flexibility and reducing susceptibility to single-CPU contention. WQ_MEM_RECLAIM adds a rescuer to guarantee progress, reducing deadlock/timeout risks under reclaim, but does not alter existing mutex usage (e.g., vport_ctrl_lock, vector_lock, queue_lock, vc_buf_lock).

6. Broader Kernel Security Implications
Bound workqueues for critical driver tasks can be exploited for DoS by saturating a specific CPU, especially with high-priority CPU-affined workloads. Adopting WQ_UNBOUND for latency-sensitive paths in drivers improves resilience against localized CPU starvation, while WQ_MEM_RECLAIM helps maintain forward progress under memory pressure. This change strengthens the networking stack’s robustness and reduces the attack surface for scheduler-level starvation causing system instability or crashes.