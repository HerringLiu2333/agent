You are a senior static-analysis engineer and CodeQL design expert.
Task: Based only on the supplied materials, produce a human-readable detection plan (natural-language) for a CodeQL checker that would detect the same class of vulnerability described. DO NOT produce any query code or pseudocode — the output must be purely natural language steps and rationale.

Rules:
1. Only use evidence contained in the sections labeled [PATCH_DESCRIPTION], [PATCH_DIFF], [FUNCTION_CONTENT], and [ROOTCAUSE_ANALYSIS]. Do not invent facts.
2. The plan must be a step-by-step detection strategy in natural language (numbered steps). No code, no query snippets, no regex, no domain-specific language.
3. Each step must state: objective, signals to look for (AST/semantic patterns expressed conceptually), why it maps to the root cause, and how to reduce false positives.
4. Include a short test/validation strategy (what sample cases to run and expected outcomes).
5. List limitations and assumptions (what is Not determinable from inputs).
6. Keep the plan concise: overall ≤ 12 numbered steps and each step ≤ 2 concise sentences. Use bullets where helpful.
7. If information required to design an accurate checker is missing, state it explicitly under "Limitations & Assumptions".

[META]
CVE_NAME: CVE-2024-58055

[INFO]
[PATCH_DESCRIPTION]
 vmxnet3: Fix packet corruption in vmxnet3_xdp_xmit_frame
 [ Upstream commit 4678adf94da4a9e9683817b246b58ce15fb81782 ]
 
 Andrew and Nikolay reported connectivity issues with Cilium's service
 load-balancing in case of vmxnet3.
 
 If a BPF program for native XDP adds an encapsulation header such as
 IPIP and transmits the packet out the same interface, then in case
 of vmxnet3 a corrupted packet is being sent and subsequently dropped
 on the path.

 vmxnet3_xdp_xmit_frame() which is called e.g. via vmxnet3_run_xdp()
 through vmxnet3_xdp_xmit_back() calculates an incorrect DMA address:
 
   page = virt_to_page(xdpf->data);
   tbi->dma_addr = page_pool_get_dma_addr(page) +
                   VMXNET3_XDP_HEADROOM;
   dma_sync_single_for_device(&adapter->pdev->dev,
                              tbi->dma_addr, buf_size,
                              DMA_TO_DEVICE);
 
 The above assumes a fixed offset (VMXNET3_XDP_HEADROOM), but the XDP
 BPF program could have moved xdp->data. While the passed buf_size is
 correct (xdpf->len), the dma_addr needs to have a dynamic offset which
 can be calculated as xdpf->data - (void *)xdpf, that is, xdp->data -
 xdp->data_hard_start.

 Fixes: 54f00cce1178 ("vmxnet3: Add XDP support.")

[PATCH_DIFF]
    diff --git a/drivers/usb/gadget/function/f_tcm.c b/drivers/usb/gadget/function/f_tcm.c
    index 81b6d0d18a8cd6..948888316fbd54 100644
    --- a/drivers/usb/gadget/function/f_tcm.c
    +++ b/drivers/usb/gadget/function/f_tcm.c
    @@ -1066,7 +1066,6 @@ static void usbg_cmd_work(struct work_struct *work)
     out:
     	transport_send_check_condition_and_sense(se_cmd,
     			TCM_UNSUPPORTED_SCSI_OPCODE, 1);
    -	transport_generic_free_cmd(&cmd->se_cmd, 0);
     }
     
     static struct usbg_cmd *usbg_get_cmd(struct f_uas *fu,
    @@ -1195,7 +1194,6 @@ static void bot_cmd_work(struct work_struct *work)
     out:
     	transport_send_check_condition_and_sense(se_cmd,
     				TCM_UNSUPPORTED_SCSI_OPCODE, 1);
    -	transport_generic_free_cmd(&cmd->se_cmd, 0);
     }
     
     static int bot_submit_command(struct f_uas *fu,

[FUNCTION_CONTENT]
static void usbg_cmd_work(struct work_struct *work)
{
	struct usbg_cmd *cmd = container_of(work, struct usbg_cmd, work);
	struct se_cmd *se_cmd;
	struct tcm_usbg_nexus *tv_nexus;
	struct usbg_tpg *tpg;
	int dir, flags = (TARGET_SCF_UNKNOWN_SIZE | TARGET_SCF_ACK_KREF);

	se_cmd = &cmd->se_cmd;
	tpg = cmd->fu->tpg;
	tv_nexus = tpg->tpg_nexus;
	dir = get_cmd_dir(cmd->cmd_buf);
	if (dir < 0) {
		__target_init_cmd(se_cmd,
				  tv_nexus->tvn_se_sess->se_tpg->se_tpg_tfo,
				  tv_nexus->tvn_se_sess, cmd->data_len, DMA_NONE,
				  cmd->prio_attr, cmd->sense_iu.sense,
				  cmd->unpacked_lun, NULL);
		goto out;
	}

	target_submit_cmd(se_cmd, tv_nexus->tvn_se_sess, cmd->cmd_buf,
			  cmd->sense_iu.sense, cmd->unpacked_lun, 0,
			  cmd->prio_attr, dir, flags);
	return;

out:
	transport_send_check_condition_and_sense(se_cmd,
			TCM_UNSUPPORTED_SCSI_OPCODE, 1);
	transport_generic_free_cmd(&cmd->se_cmd, 0);
}

/* ----- separator ----- */

static struct usbg_cmd *usbg_get_cmd(struct f_uas *fu,
		struct tcm_usbg_nexus *tv_nexus, u32 scsi_tag)
{
	struct se_session *se_sess = tv_nexus->tvn_se_sess;
	struct usbg_cmd *cmd;
	int tag, cpu;

	tag = sbitmap_queue_get(&se_sess->sess_tag_pool, &cpu);
	if (tag < 0)
		return ERR_PTR(-ENOMEM);

	cmd = &((struct usbg_cmd *)se_sess->sess_cmd_map)[tag];
	memset(cmd, 0, sizeof(*cmd));
	cmd->se_cmd.map_tag = tag;
	cmd->se_cmd.map_cpu = cpu;
	cmd->se_cmd.tag = cmd->tag = scsi_tag;
	cmd->fu = fu;

	return cmd;
}

/* ----- separator ----- */

static void bot_cmd_work(struct work_struct *work)
{
	struct usbg_cmd *cmd = container_of(work, struct usbg_cmd, work);
	struct se_cmd *se_cmd;
	struct tcm_usbg_nexus *tv_nexus;
	struct usbg_tpg *tpg;
	int dir;

	se_cmd = &cmd->se_cmd;
	tpg = cmd->fu->tpg;
	tv_nexus = tpg->tpg_nexus;
	dir = get_cmd_dir(cmd->cmd_buf);
	if (dir < 0) {
		__target_init_cmd(se_cmd,
				  tv_nexus->tvn_se_sess->se_tpg->se_tpg_tfo,
				  tv_nexus->tvn_se_sess, cmd->data_len, DMA_NONE,
				  cmd->prio_attr, cmd->sense_iu.sense,
				  cmd->unpacked_lun, NULL);
		goto out;
	}

	target_submit_cmd(se_cmd, tv_nexus->tvn_se_sess,
			  cmd->cmd_buf, cmd->sense_iu.sense, cmd->unpacked_lun,
			  cmd->data_len, cmd->prio_attr, dir, 0);
	return;

out:
	transport_send_check_condition_and_sense(se_cmd,
				TCM_UNSUPPORTED_SCSI_OPCODE, 1);
	transport_generic_free_cmd(&cmd->se_cmd, 0);
}

/* ----- separator ----- */

static int bot_submit_command(struct f_uas *fu,
		void *cmdbuf, unsigned int len)
{
	struct bulk_cb_wrap *cbw = cmdbuf;
	struct usbg_cmd *cmd;
	struct usbg_tpg *tpg = fu->tpg;
	struct tcm_usbg_nexus *tv_nexus;
	u32 cmd_len;

	if (cbw->Signature != cpu_to_le32(US_BULK_CB_SIGN)) {
		pr_err("Wrong signature on CBW\n");
		return -EINVAL;
	}
	if (len != 31) {
		pr_err("Wrong length for CBW\n");
		return -EINVAL;
	}

	cmd_len = cbw->Length;
	if (cmd_len < 1 || cmd_len > 16)
		return -EINVAL;

	tv_nexus = tpg->tpg_nexus;
	if (!tv_nexus) {
		pr_err("Missing nexus, ignoring command\n");
		return -ENODEV;
	}

	cmd = usbg_get_cmd(fu, tv_nexus, cbw->Tag);
	if (IS_ERR(cmd)) {
		pr_err("usbg_get_cmd failed\n");
		return -ENOMEM;
	}
	memcpy(cmd->cmd_buf, cbw->CDB, cmd_len);

	cmd->bot_tag = cbw->Tag;
	cmd->prio_attr = TCM_SIMPLE_TAG;
	cmd->unpacked_lun = cbw->Lun;
	cmd->is_read = cbw->Flags & US_BULK_FLAG_IN ? 1 : 0;
	cmd->data_len = le32_to_cpu(cbw->DataTransferLength);
	cmd->se_cmd.tag = le32_to_cpu(cmd->bot_tag);

	INIT_WORK(&cmd->work, bot_cmd_work);
	queue_work(tpg->workqueue, &cmd->work);

	return 0;
}

/* ----- separator ----- */


	INIT_WORK(&cmd->work, usbg_cmd_work);
	queue_work(tpg->workqueue, &cmd->work);

	return 0;
err:
	usbg_release_cmd(&cmd->se_cmd);
	return -EINVAL;
}

static void bot_cmd_work(struct work_struct *work)
{
	struct usbg_cmd *cmd = container_of(work, struct usbg_cmd, work);
	struct se_cmd *se_cmd;
	struct tcm_usbg_nexus *tv_nexus;
	struct usbg_tpg *tpg;
	int dir;

	se_cmd = &cmd->se_cmd;
	tpg = cmd->fu->tpg;
	tv_nexus = tpg->tpg_nexus;
	dir = get_cmd_dir(cmd->cmd_buf);
	if (dir < 0) {
		__target_init_cmd(se_cmd,
				  tv_nexus->tvn_se_sess->se_tpg->se_tpg_tfo,
				  tv_nexus->tvn_se_sess, cmd->data_len, DMA_NONE,
				  cmd->prio_attr, cmd->sense_iu.sense,
				  cmd->unpacked_lun, NULL);
		goto out;
	}

	target_submit_cmd(se_cmd, tv_nexus->tvn_se_sess,
			  cmd->cmd_buf, cmd->sense_iu.sense, cmd->unpacked_lun,
			  cmd->data_len, cmd->prio_attr, dir, 0);
	return;

out:
	transport_send_check_condition_and_sense(se_cmd,
				TCM_UNSUPPORTED_SCSI_OPCODE, 1);
	transport_generic_free_cmd(&cmd->se_cmd, 0);
}



[ROOTCAUSE_ANALYSIS]
1. CVE Identifier
CVE-2024-58055

2. Vulnerability Type
Incorrect DMA address calculation leading to packet data corruption

3. Root Cause Summary
vmxnet3_xdp_xmit_frame computed the DMA address as page_pool_get_dma_addr(virt_to_page(xdpf->data)) + VMXNET3_XDP_HEADROOM, assuming a fixed headroom offset. Native XDP programs can move xdpf->data (e.g., when adding encapsulation headers), so the hard-coded headroom no longer matches the actual buffer start. As a result, the NIC was instructed to DMA from the wrong offset within the page, producing corrupted packets even though the buffer length (xdpf->len) was correct. The correct offset must be derived dynamically from the current data pointer, i.e., xdpf->data - xdpf->data_hard_start.

4. Kernel Subsystem Analysis
1) Affected Subsystem:
Networking driver vmxnet3 XDP transmit path (vmxnet3_xdp_xmit_frame)

2) Pre-Patch Flaw:
The transmit routine assumed a static headroom (VMXNET3_XDP_HEADROOM) when computing tbi->dma_addr, failing to account for XDP programs that adjust xdpf->data. This mismatch between data pointer and DMA address produced misaligned DMA reads of packet data.

3) Trigger Condition:
Running a native XDP program that modifies the packet head (e.g., adds IPIP encapsulation) and transmits on the same vmxnet3 interface, invoking vmxnet3_xdp_xmit_frame via vmxnet3_run_xdp/vmxnet3_xdp_xmit_back.

4) Impact Mechanism:
The NIC DMA fetches start at an incorrect memory address, causing the frame contents to be corrupted. Corrupted packets are subsequently dropped along the network path, resulting in connectivity issues.

5. Patch Analysis
1) Fix Approach:
Compute tbi->dma_addr using a dynamic offset based on the current xdp buffer data pointer: page_pool_get_dma_addr(page) + (xdpf->data - xdpf->data_hard_start). Keep buf_size as xdpf->len and perform dma_sync_single_for_device on the corrected address.

2) Key Code Changes:
- Replace addition of VMXNET3_XDP_HEADROOM with a calculated offset: xdpf->data - (void *)xdpf or xdpf->data - xdpf->data_hard_start.
- Set tbi->dma_addr accordingly before dma_sync_single_for_device.

3) Locking/Concurrency Impact:
No locking or concurrency changes; the fix strictly corrects address calculation and DMA mapping semantics.

6. Broader Kernel Security Implications
Incorrect DMA address calculations in XDP-capable drivers can silently corrupt transmitted data, leading to denial-of-service via packet drops and potential data integrity issues. Ensuring DMA mappings respect dynamic XDP buffer pointer semantics (data_hard_start/data) is critical for correctness and for preventing subtle bugs that can undermine network reliability and, in some contexts, risk leakage of unintended memory contents.

[REQUEST]
Produce a detection plan for a CodeQL-based static checker that would detect similar pre-patch flaws.
Requirements for the plan:
- High-level detection goal (1–2 lines).
- A numbered list of detection steps (objective, conceptual AST/semantic signals, FP mitigation).
- Types of program elements to target (functions, call sites, allocation sites, condition checks, lock boundaries, function return-value uses, etc.).
- Dataflow/taint patterns to consider (if applicable), described conceptually.
- Minimal test cases to validate the checker (positive/negative examples).
- Estimated effort/priority (low/medium/high) and likely false-positive sources.
- A short "Limitations & Assumptions" block.

OUTPUT FORMAT (produce exactly this structure; no extra text):
1. Plan Summary
{one-line summary}

2. Detection Steps
1) Step 1: {objective — conceptual signals — FP mitigation}
2) Step 2: {objective — conceptual signals — FP mitigation}
...
(narrow to ≤12 steps)

3. Target Elements
- {list of element types to inspect}

4. Dataflow / Taint Considerations
- {conceptual taint/flow rules to track}

5. Validation & Test Cases
- Positive: {brief}
- Negative: {brief}
- Test harness notes: {brief}

6. Estimated Effort & Priority
{low/medium/high}

7. Likely False-Positive Sources & Mitigations
- {list}

8. Limitations & Assumptions
- {explicit missing info or assumptions}

CONSTRAINTS:
- Do not emit any CodeQL, SQL, pseudocode, or query fragments.
- Keep answers evidence-based and reference which provided field supported each major choice (e.g., “based on [PATCH_DIFF] hunk that adds X”).
- Output must be machine-parseable: keep the exact numbered section headings as above.