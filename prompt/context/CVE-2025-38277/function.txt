static int mxic_ecc_finish_io_req_external(struct nand_device *nand,
					   struct nand_page_io_req *req)
{
	struct mxic_ecc_engine *mxic = nand_to_mxic(nand);
	struct mxic_ecc_ctx *ctx = nand_to_ecc_ctx(nand);
	int nents, step, ret;

	if (req->mode == MTD_OPS_RAW)
		return 0;

	if (req->type == NAND_PAGE_WRITE) {
		nand_ecc_restore_req(&ctx->req_ctx, req);
		return 0;
	}

	/* Copy the OOB buffer and add room for the ECC engine status bytes */
	mxic_ecc_add_room_in_oobbuf(ctx, ctx->oobwithstat, ctx->req->oobbuf.in);

	sg_set_buf(&ctx->sg[0], req->databuf.in, req->datalen);
	sg_set_buf(&ctx->sg[1], ctx->oobwithstat,
		   req->ooblen + (ctx->steps * STAT_BYTES));
	nents = dma_map_sg(mxic->dev, ctx->sg, 2, DMA_BIDIRECTIONAL);
	if (!nents)
		return -EINVAL;

	mutex_lock(&mxic->lock);

	for (step = 0; step < ctx->steps; step++) {
		writel(sg_dma_address(&ctx->sg[0]) + (step * ctx->data_step_sz),
		       mxic->regs + SDMA_MAIN_ADDR);
		writel(sg_dma_address(&ctx->sg[1]) + (step * (ctx->oob_step_sz + STAT_BYTES)),
		       mxic->regs + SDMA_SPARE_ADDR);
		ret = mxic_ecc_process_data(mxic, ctx->req->type);
		if (ret)
			break;
	}

	mutex_unlock(&mxic->lock);

	dma_unmap_sg(mxic->dev, ctx->sg, 2, DMA_BIDIRECTIONAL);

	if (ret) {
		nand_ecc_restore_req(&ctx->req_ctx, req);
		return ret;
	}

	/* Extract the status bytes and reconstruct the buffer */
	mxic_ecc_extract_status_bytes(ctx);
	mxic_ecc_reconstruct_oobbuf(ctx, ctx->req->oobbuf.in, ctx->oobwithstat);

	nand_ecc_restore_req(&ctx->req_ctx, req);

	return mxic_ecc_count_biterrs(mxic, nand);
}