You are a senior static-analysis engineer and CodeQL design expert.
Task: Based only on the supplied materials, produce a human-readable detection plan (natural-language) for a CodeQL checker that would detect the same class of vulnerability described. DO NOT produce any query code or pseudocode — the output must be purely natural language steps and rationale.

Rules:
1. Only use evidence contained in the sections labeled [PATCH_DESCRIPTION], [PATCH_DIFF], [FUNCTION_CONTENT], and [ROOTCAUSE_ANALYSIS]. Do not invent facts.
2. The plan must be a step-by-step detection strategy in natural language (numbered steps). No code, no query snippets, no regex, no domain-specific language.
3. Each step must state: objective, signals to look for (AST/semantic patterns expressed conceptually), why it maps to the root cause, and how to reduce false positives.
4. Include a short test/validation strategy (what sample cases to run and expected outcomes).
5. List limitations and assumptions (what is Not determinable from inputs).
6. Keep the plan concise: overall ≤ 12 numbered steps and each step ≤ 2 concise sentences. Use bullets where helpful.
7. If information required to design an accurate checker is missing, state it explicitly under "Limitations & Assumptions".

[META]
CVE_NAME: CVE-2025-38269

[INFO]
[PATCH_DESCRIPTION]
    btrfs: exit after state insertion failure at btrfs_convert_extent_bit()
    If insert_state() state failed it returns an error pointer and we call
    extent_io_tree_panic() which will trigger a BUG() call. However if
    CONFIG_BUG is disabled, which is an uncommon and exotic scenario, then
    we fallthrough and call cache_state() which will dereference the error
    pointer, resulting in an invalid memory access.
    
    So jump to the 'out' label after calling extent_io_tree_panic(), it also
    makes the code more clear besides dealing with the exotic scenario where
    CONFIG_BUG is disabled.

[PATCH_DIFF]
    @@ -1439,6 +1439,7 @@ hit_next:
     		if (IS_ERR(inserted_state)) {
     			ret = PTR_ERR(inserted_state);
     			extent_io_tree_panic(tree, prealloc, "insert", ret);
    +			goto out;
     		}
     		cache_state(inserted_state, cached_state);
     		if (inserted_state == prealloc)

[FUNCTION_CONTENT]
int btrfs_convert_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,
			     u32 bits, u32 clear_bits,
			     struct extent_state **cached_state)
{
	struct extent_state *state;
	struct extent_state *prealloc = NULL;
	struct rb_node **p = NULL;
	struct rb_node *parent = NULL;
	int ret = 0;
	u64 last_start;
	u64 last_end;
	bool first_iteration = true;

	btrfs_debug_check_extent_io_range(tree, start, end);
	trace_btrfs_convert_extent_bit(tree, start, end - start + 1, bits,
				       clear_bits);

again:
	if (!prealloc) {
		/*
		 * Best effort, don't worry if extent state allocation fails
		 * here for the first iteration. We might have a cached state
		 * that matches exactly the target range, in which case no
		 * extent state allocations are needed. We'll only know this
		 * after locking the tree.
		 */
		prealloc = alloc_extent_state(GFP_NOFS);
		if (!prealloc && !first_iteration)
			return -ENOMEM;
	}

	spin_lock(&tree->lock);
	if (cached_state && *cached_state) {
		state = *cached_state;
		if (state->start <= start && state->end > start &&
		    extent_state_in_tree(state))
			goto hit_next;
	}

	/*
	 * This search will find all the extents that end after our range
	 * starts.
	 */
	state = tree_search_for_insert(tree, start, &p, &parent);
	if (!state) {
		prealloc = alloc_extent_state_atomic(prealloc);
		if (!prealloc) {
			ret = -ENOMEM;
			goto out;
		}
		prealloc->start = start;
		prealloc->end = end;
		insert_state_fast(tree, prealloc, p, parent, bits, NULL);
		cache_state(prealloc, cached_state);
		prealloc = NULL;
		goto out;
	}
hit_next:
	last_start = state->start;
	last_end = state->end;

	/*
	 * | ---- desired range ---- |
	 * | state |
	 *
	 * Just lock what we found and keep going.
	 */
	if (state->start == start && state->end <= end) {
		set_state_bits(tree, state, bits, NULL);
		cache_state(state, cached_state);
		state = clear_state_bit(tree, state, clear_bits, 0, NULL);
		if (last_end == (u64)-1)
			goto out;
		start = last_end + 1;
		if (start < end && state && state->start == start &&
		    !need_resched())
			goto hit_next;
		goto search_again;
	}

	/*
	 *     | ---- desired range ---- |
	 * | state |
	 *   or
	 * | ------------- state -------------- |
	 *
	 * We need to split the extent we found, and may flip bits on second
	 * half.
	 *
	 * If the extent we found extends past our range, we just split and
	 * search again.  It'll get split again the next time though.
	 *
	 * If the extent we found is inside our range, we set the desired bit
	 * on it.
	 */
	if (state->start < start) {
		prealloc = alloc_extent_state_atomic(prealloc);
		if (!prealloc) {
			ret = -ENOMEM;
			goto out;
		}
		ret = split_state(tree, state, prealloc, start);
		prealloc = NULL;
		if (ret) {
			extent_io_tree_panic(tree, state, "split", ret);
			goto out;
		}
		if (state->end <= end) {
			set_state_bits(tree, state, bits, NULL);
			cache_state(state, cached_state);
			state = clear_state_bit(tree, state, clear_bits, 0, NULL);
			if (last_end == (u64)-1)
				goto out;
			start = last_end + 1;
			if (start < end && state && state->start == start &&
			    !need_resched())
				goto hit_next;
		}
		goto search_again;
	}
	/*
	 * | ---- desired range ---- |
	 *     | state | or               | state |
	 *
	 * There's a hole, we need to insert something in it and ignore the
	 * extent we found.
	 */
	if (state->start > start) {
		u64 this_end;
		struct extent_state *inserted_state;

		if (end < last_start)
			this_end = end;
		else
			this_end = last_start - 1;

		prealloc = alloc_extent_state_atomic(prealloc);
		if (!prealloc) {
			ret = -ENOMEM;
			goto out;
		}

		/*
		 * Avoid to free 'prealloc' if it can be merged with the later
		 * extent.
		 */
		prealloc->start = start;
		prealloc->end = this_end;
		inserted_state = insert_state(tree, prealloc, bits, NULL);
		if (IS_ERR(inserted_state)) {
			ret = PTR_ERR(inserted_state);
			extent_io_tree_panic(tree, prealloc, "insert", ret);
		}
		cache_state(inserted_state, cached_state);
		if (inserted_state == prealloc)
			prealloc = NULL;
		start = this_end + 1;
		goto search_again;
	}
	/*
	 * | ---- desired range ---- |
	 *                        | state |
	 *
	 * We need to split the extent, and set the bit on the first half.
	 */
	if (state->start <= end && state->end > end) {
		prealloc = alloc_extent_state_atomic(prealloc);
		if (!prealloc) {
			ret = -ENOMEM;
			goto out;
		}

		ret = split_state(tree, state, prealloc, end + 1);
		if (ret) {
			extent_io_tree_panic(tree, state, "split", ret);
			prealloc = NULL;
			goto out;
		}

		set_state_bits(tree, prealloc, bits, NULL);
		cache_state(prealloc, cached_state);
		clear_state_bit(tree, prealloc, clear_bits, 0, NULL);
		prealloc = NULL;
		goto out;
	}

search_again:
	if (start > end)
		goto out;
	spin_unlock(&tree->lock);
	cond_resched();
	first_iteration = false;
	goto again;

out:
	spin_unlock(&tree->lock);
	if (prealloc)
		btrfs_free_extent_state(prealloc);

	return ret;
}



[ROOTCAUSE_ANALYSIS]
1. CVE Identifier
CVE-2025-38269

2. Vulnerability Type
Improper error handling leading to ERR_PTR dereference (invalid pointer dereference)

3. Root Cause Summary
The function btrfs_convert_extent_bit() failed to exit after detecting an insert_state() error and calling extent_io_tree_panic(), causing it to fall through and dereference the error pointer returned by insert_state(). Specifically, in the “hole insertion” path (state->start > start), when insert_state() returned IS_ERR(inserted_state), the code executed extent_io_tree_panic(tree, prealloc, "insert", ret) but then immediately called cache_state(inserted_state, cached_state), dereferencing an ERR_PTR. This control-flow bug was masked when CONFIG_BUG is enabled (BUG() halts execution), but caused an invalid memory access when CONFIG_BUG is disabled.

4. Kernel Subsystem Analysis
1) Affected Subsystem:
Btrfs extent I/O tree state management (fs/btrfs, extent_io_tree handling)

2) Pre-Patch Flaw:
In btrfs_convert_extent_bit(), after a failed insert_state() in the state->start > start branch, the code did not abort the operation. It called extent_io_tree_panic() and then continued to cache_state(inserted_state, cached_state) without verifying or handling the error pointer.

3) Trigger Condition:
insert_state(tree, prealloc, bits, NULL) returns an ERR_PTR due to insertion failure (e.g., memory allocation failure or tree invariants), and CONFIG_BUG is disabled so extent_io_tree_panic() does not terminate execution.

4) Impact Mechanism:
cache_state() dereferences the ERR_PTR (inserted_state), leading to invalid memory access under a held spinlock, resulting in a kernel crash/Oops (and potentially leaving locks held if the crash occurs before unlocking).

5. Patch Analysis
1) Fix Approach:
Ensure an immediate exit from the error path by jumping to the common out label after calling extent_io_tree_panic(), preventing use of the error pointer and guaranteeing proper cleanup and lock release.

2) Key Code Changes:
Added a single control-flow statement in the error branch:
- After ret = PTR_ERR(inserted_state); extent_io_tree_panic(tree, prealloc, "insert", ret);
- Inserted goto out; to avoid calling cache_state(inserted_state, cached_state) when inserted_state is an ERR_PTR.

3) Locking/Concurrency Impact:
The added goto out ensures tree->lock is released and prealloc is freed via the out path on error, improving error-path lock discipline. Previously, if execution continued past extent_io_tree_panic() (CONFIG_BUG disabled), the code could dereference an invalid pointer while holding the spinlock, leading to a crash with the lock still held.

6. Broader Kernel Security Implications
This fix reinforces robust error handling independent of BUG() semantics, ensuring safety even in configurations where BUG is disabled. Preventing ERR_PTR dereferences mitigates kernel crashes that can be triggered by low-memory or insertion failures. It highlights the importance of consistent early-exit patterns in error paths to avoid invalid memory accesses and lock leaks in critical filesystem code.

[REQUEST]
Produce a detection plan for a CodeQL-based static checker that would detect similar pre-patch flaws.
Requirements for the plan:
- High-level detection goal (1–2 lines).
- A numbered list of detection steps (objective, conceptual AST/semantic signals, FP mitigation).
- Types of program elements to target (functions, call sites, allocation sites, condition checks, lock boundaries, function return-value uses, etc.).
- Dataflow/taint patterns to consider (if applicable), described conceptually.
- Minimal test cases to validate the checker (positive/negative examples).
- Estimated effort/priority (low/medium/high) and likely false-positive sources.
- A short "Limitations & Assumptions" block.

OUTPUT FORMAT (produce exactly this structure; no extra text):
1. Plan Summary
{one-line summary}

2. Detection Steps
1) Step 1: {objective — conceptual signals — FP mitigation}
2) Step 2: {objective — conceptual signals — FP mitigation}
...
(narrow to ≤12 steps)

3. Target Elements
- {list of element types to inspect}

4. Dataflow / Taint Considerations
- {conceptual taint/flow rules to track}

5. Validation & Test Cases
- Positive: {brief}
- Negative: {brief}
- Test harness notes: {brief}

6. Estimated Effort & Priority
{low/medium/high}

7. Likely False-Positive Sources & Mitigations
- {list}

8. Limitations & Assumptions
- {explicit missing info or assumptions}

CONSTRAINTS:
- Do not emit any CodeQL, SQL, pseudocode, or query fragments.
- Keep answers evidence-based and reference which provided field supported each major choice (e.g., “based on [PATCH_DIFF] hunk that adds X”).
- Output must be machine-parseable: keep the exact numbered section headings as above.