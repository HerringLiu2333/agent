You are a senior static-analysis engineer and CodeQL design expert.
Task: Based only on the supplied materials, produce a human-readable detection plan (natural-language) for a CodeQL checker that would detect the same class of vulnerability described. DO NOT produce any query code or pseudocode — the output must be purely natural language steps and rationale.

Rules:
1. Only use evidence contained in the sections labeled [PATCH_DESCRIPTION], [PATCH_DIFF], [FUNCTION_CONTENT], and [ROOTCAUSE_ANALYSIS]. Do not invent facts.
2. The plan must be a step-by-step detection strategy in natural language (numbered steps). No code, no query snippets, no regex, no domain-specific language.
3. Each step must state: objective, signals to look for (AST/semantic patterns expressed conceptually), why it maps to the root cause, and how to reduce false positives.
4. Include a short test/validation strategy (what sample cases to run and expected outcomes).
5. List limitations and assumptions (what is Not determinable from inputs).
6. Keep the plan concise: overall ≤ 12 numbered steps and each step ≤ 2 concise sentences. Use bullets where helpful.
7. If information required to design an accurate checker is missing, state it explicitly under "Limitations & Assumptions".

[META]
CVE_NAME: CVE-2025-38349

[INFO]
[PATCH_DESCRIPTION]
    eventpoll: don't decrement ep refcount while still holding the ep mutex
    Jann Horn points out that epoll is decrementing the ep refcount and then
    doing a
    
        mutex_unlock(&ep->mtx);
    
    afterwards. That's very wrong, because it can lead to a use-after-free.
    
    That pattern is actually fine for the very last reference, because the
    code in question will delay the actual call to "ep_free(ep)" until after
    it has unlocked the mutex.
    
    But it's wrong for the much subtler "next to last" case when somebody
    *else* may also be dropping their reference and free the ep while we're
    still using the mutex.
    
    Note that this is true even if that other user is also using the same ep
    mutex: mutexes, unlike spinlocks, can not be used for object ownership,
    even if they guarantee mutual exclusion.
    
    A mutex "unlock" operation is not atomic, and as one user is still
    accessing the mutex as part of unlocking it, another user can come in
    and get the now released mutex and free the data structure while the
    first user is still cleaning up.
    
    See our mutex documentation in Documentation/locking/mutex-design.rst,
    in particular the section [1] about semantics:
    
    	"mutex_unlock() may access the mutex structure even after it has
    	 internally released the lock already - so it's not safe for
    	 another context to acquire the mutex and assume that the
    	 mutex_unlock() context is not using the structure anymore"
    
    So if we drop our ep ref before the mutex unlock, but we weren't the
    last one, we may then unlock the mutex, another user comes in, drops
    _their_ reference and releases the 'ep' as it now has no users - all
    while the mutex_unlock() is still accessing it.
    
    Fix this by simply moving the ep refcount dropping to outside the mutex:
    the refcount itself is atomic, and doesn't need mutex protection (that's
    the whole _point_ of refcounts: unlike mutexes, they are inherently
    about object lifetimes).

[PATCH_DIFF]
    diff --git a/fs/eventpoll.c b/fs/eventpoll.c
    index a97a771a459c9c..895256cd2786e2 100644
    --- a/fs/eventpoll.c
    +++ b/fs/eventpoll.c
    @@ -828,7 +828,7 @@ static bool __ep_remove(struct eventpoll *ep, struct epitem *epi, bool force)
     	kfree_rcu(epi, rcu);
     
     	percpu_counter_dec(&ep->user->epoll_watches);
    -	return ep_refcount_dec_and_test(ep);
    +	return true;
     }
     
     /*
    @@ -836,14 +836,14 @@ static bool __ep_remove(struct eventpoll *ep, struct epitem *epi, bool force)
      */
     static void ep_remove_safe(struct eventpoll *ep, struct epitem *epi)
     {
    -	WARN_ON_ONCE(__ep_remove(ep, epi, false));
    +	if (__ep_remove(ep, epi, false))
    +		WARN_ON_ONCE(ep_refcount_dec_and_test(ep));
     }
     
     static void ep_clear_and_put(struct eventpoll *ep)
     {
     	struct rb_node *rbp, *next;
     	struct epitem *epi;
    -	bool dispose;
     
     	/* We need to release all tasks waiting for these file */
     	if (waitqueue_active(&ep->poll_wait))
    @@ -876,10 +876,8 @@ static void ep_clear_and_put(struct eventpoll *ep)
     		cond_resched();
     	}
     
    -	dispose = ep_refcount_dec_and_test(ep);
     	mutex_unlock(&ep->mtx);
    -
    -	if (dispose)
    +	if (ep_refcount_dec_and_test(ep))
     		ep_free(ep);
     }
     
    @@ -1100,7 +1098,7 @@ again:
     		dispose = __ep_remove(ep, epi, true);
     		mutex_unlock(&ep->mtx);
     
    -		if (dispose)
    +		if (dispose && ep_refcount_dec_and_test(ep))
     			ep_free(ep);
     		goto again;
     	}

[FUNCTION_CONTENT]
static bool __ep_remove(struct eventpoll *ep, struct epitem *epi, bool force)
{
	struct file *file = epi->ffd.file;
	struct llist_node *put_back_last;
	struct epitems_head *to_free;
	struct hlist_head *head;
	LLIST_HEAD(put_back);

	lockdep_assert_held(&ep->mtx);

	/*
	 * Removes poll wait queue hooks.
	 */
	ep_unregister_pollwait(ep, epi);

	/* Remove the current item from the list of epoll hooks */
	spin_lock(&file->f_lock);
	if (epi->dying && !force) {
		spin_unlock(&file->f_lock);
		return false;
	}

	to_free = NULL;
	head = file->f_ep;
	if (head->first == &epi->fllink && !epi->fllink.next) {
		/* See eventpoll_release() for details. */
		WRITE_ONCE(file->f_ep, NULL);
		if (!is_file_epoll(file)) {
			struct epitems_head *v;
			v = container_of(head, struct epitems_head, epitems);
			if (!smp_load_acquire(&v->next))
				to_free = v;
		}
	}
	hlist_del_rcu(&epi->fllink);
	spin_unlock(&file->f_lock);
	free_ephead(to_free);

	rb_erase_cached(&epi->rbn, &ep->rbr);

	if (llist_on_list(&epi->rdllink)) {
		put_back_last = NULL;
		while (true) {
			struct llist_node *n = llist_del_first(&ep->rdllist);

			if (&epi->rdllink == n || WARN_ON(!n))
				break;
			if (!put_back_last)
				put_back_last = n;
			__llist_add(n, &put_back);
		}
		if (put_back_last)
			llist_add_batch(put_back.first, put_back_last, &ep->rdllist);
	}

	wakeup_source_unregister(ep_wakeup_source(epi));
	/*
	 * At this point it is safe to free the eventpoll item. Use the union
	 * field epi->rcu, since we are trying to minimize the size of
	 * 'struct epitem'. The 'rbn' field is no longer in use. Protected by
	 * ep->mtx. The rcu read side, reverse_path_check_proc(), does not make
	 * use of the rbn field.
	 */
	kfree_rcu(epi, rcu);

	percpu_counter_dec(&ep->user->epoll_watches);
	return ep_refcount_dec_and_test(ep);
}

/* ----- separator ----- */

static void ep_remove_safe(struct eventpoll *ep, struct epitem *epi)
{
	WARN_ON_ONCE(__ep_remove(ep, epi, false));
}

/* ----- separator ----- */

static void ep_clear_and_put(struct eventpoll *ep)
{
	struct rb_node *rbp, *next;
	struct epitem *epi;
	bool dispose;

	/* We need to release all tasks waiting for these file */
	if (waitqueue_active(&ep->poll_wait))
		ep_poll_safewake(ep, NULL, 0);

	mutex_lock(&ep->mtx);

	/*
	 * Walks through the whole tree by unregistering poll callbacks.
	 */
	for (rbp = rb_first_cached(&ep->rbr); rbp; rbp = rb_next(rbp)) {
		epi = rb_entry(rbp, struct epitem, rbn);

		ep_unregister_pollwait(ep, epi);
		cond_resched();
	}

	/*
	 * Walks through the whole tree and try to free each "struct epitem".
	 * Note that ep_remove_safe() will not remove the epitem in case of a
	 * racing eventpoll_release_file(); the latter will do the removal.
	 * At this point we are sure no poll callbacks will be lingering around.
	 * Since we still own a reference to the eventpoll struct, the loop can't
	 * dispose it.
	 */
	for (rbp = rb_first_cached(&ep->rbr); rbp; rbp = next) {
		next = rb_next(rbp);
		epi = rb_entry(rbp, struct epitem, rbn);
		ep_remove_safe(ep, epi);
		cond_resched();
	}

	dispose = ep_refcount_dec_and_test(ep);
	mutex_unlock(&ep->mtx);

	if (dispose)
		ep_free(ep);
}

/* ----- separator ----- */

	WARN_ON_ONCE(__ep_remove(ep, epi, false));
}

static void ep_clear_and_put(struct eventpoll *ep)
{
	struct rb_node *rbp, *next;
	struct epitem *epi;
	bool dispose;

	/* We need to release all tasks waiting for these file */
	if (waitqueue_active(&ep->poll_wait))
		ep_poll_safewake(ep, NULL, 0);

	mutex_lock(&ep->mtx);

	/*
	 * Walks through the whole tree by unregistering poll callbacks.
	 */
	for (rbp = rb_first_cached(&ep->rbr); rbp; rbp = rb_next(rbp)) {
		epi = rb_entry(rbp, struct epitem, rbn);

		ep_unregister_pollwait(ep, epi);
		cond_resched();
	}

	/*
	 * Walks through the whole tree and try to free each "struct epitem".
	 * Note that ep_remove_safe() will not remove the epitem in case of a
	 * racing eventpoll_release_file(); the latter will do the removal.
	 * At this point we are sure no poll callbacks will be lingering around.
	 * Since we still own a reference to the eventpoll struct, the loop can't
	 * dispose it.
	 */
	for (rbp = rb_first_cached(&ep->rbr); rbp; rbp = next) {
		next = rb_next(rbp);
		epi = rb_entry(rbp, struct epitem, rbn);
		ep_remove_safe(ep, epi);
		cond_resched();
	}

	dispose = ep_refcount_dec_and_test(ep);
	mutex_unlock(&ep->mtx);

	if (dispose)
		ep_free(ep);
}

/* ----- separator ----- */

void eventpoll_release_file(struct file *file)
{
	struct eventpoll *ep;
	struct epitem *epi;
	bool dispose;

	/*
	 * Use the 'dying' flag to prevent a concurrent ep_clear_and_put() from
	 * touching the epitems list before eventpoll_release_file() can access
	 * the ep->mtx.
	 */
again:
	spin_lock(&file->f_lock);
	if (file->f_ep && file->f_ep->first) {
		epi = hlist_entry(file->f_ep->first, struct epitem, fllink);
		epi->dying = true;
		spin_unlock(&file->f_lock);

		/*
		 * ep access is safe as we still own a reference to the ep
		 * struct
		 */
		ep = epi->ep;
		mutex_lock(&ep->mtx);
		dispose = __ep_remove(ep, epi, true);
		mutex_unlock(&ep->mtx);

		if (dispose)
			ep_free(ep);
		goto again;
	}
	spin_unlock(&file->f_lock);
}



[ROOTCAUSE_ANALYSIS]
1. CVE Identifier
CVE-2025-38349

2. Vulnerability Type
Use-after-free due to refcount/lifetime mismanagement with incorrect mutex usage

3. Root Cause Summary
Pre-patch code decremented the struct eventpoll reference count while ep->mtx was held, then performed mutex_unlock(&ep->mtx). In the “next-to-last reference” case, another thread could acquire the mutex, drop its own reference, and free the eventpoll object while the first thread was still executing mutex_unlock(), which may still access the mutex structure. This creates a use-after-free of the eventpoll object because mutexes cannot be used to protect object lifetime across unlock operations.

4. Kernel Subsystem Analysis
1) Affected Subsystem:
fs/eventpoll (epoll)

2) Pre-Patch Flaw:
- __ep_remove() performed ep_refcount_dec_and_test(ep) under ep->mtx and returned its result: “return ep_refcount_dec_and_test(ep);” with lockdep_assert_held(&ep->mtx).
- ep_clear_and_put() dropped the ep refcount before unlocking: “dispose = ep_refcount_dec_and_test(ep); mutex_unlock(&ep->mtx);”.
- eventpoll_release_file() relied on __ep_remove() to decrement the refcount under the mutex, then unlocked and freed based on that result: “dispose = __ep_remove(ep, epi, true); mutex_unlock(&ep->mtx); if (dispose) ep_free(ep);”.
These patterns incorrectly used the mutex to guard object lifetime, violating documented mutex semantics.

3) Trigger Condition:
Concurrent operations on the same eventpoll instance where one thread drops the “next-to-last” reference under ep->mtx and then calls mutex_unlock(), while a second thread acquires the mutex, drops the last reference, and frees the eventpoll object. The first thread’s mutex_unlock() still accesses the freed ep->mtx, leading to UAF.

4) Impact Mechanism:
Use-after-free of struct eventpoll during mutex_unlock(), potentially causing memory corruption, kernel crashes, or other undefined behavior when the freed epoll structure (or its mutex) is touched during unlock cleanup.

5. Patch Analysis
1) Fix Approach:
Move all ep refcount decrement operations out from under ep->mtx and perform them after mutex_unlock(). Remove refcount manipulation from __ep_remove(). Ensure ep_free(ep) is only called after refcount decrement outside the mutex, eliminating the unlock/UAF window.

2) Key Code Changes:
- __ep_remove(): changed from “return ep_refcount_dec_and_test(ep);” to “return true;”, removing refcount decrement under the mutex.
- ep_remove_safe(): now separates removal from refcount handling; it uses “if (__ep_remove(ep, epi, false)) WARN_ON_ONCE(ep_refcount_dec_and_test(ep));” to preserve the warning semantics while no longer tying refcount changes to __ep_remove().
- ep_clear_and_put(): moved ep_refcount_dec_and_test(ep) to after mutex_unlock(&ep->mtx), i.e., “mutex_unlock(&ep->mtx); if (ep_refcount_dec_and_test(ep)) ep_free(ep);”.
- eventpoll_release_file(): after __ep_remove() and mutex_unlock(), it now performs “if (dispose && ep_refcount_dec_and_test(ep)) ep_free(ep);”, ensuring the refcount drop and potential free are outside the mutex.

3) Locking/Concurrency Impact:
- Eliminates the race where ep is freed between refcount decrement and mutex unlock by performing the decrement after unlocking.
- Aligns with mutex design semantics: no reliance on mutexes for object lifetime across unlock; uses atomic refcount for lifetime control.
- Prevents other threads from freeing ep while the current thread is still accessing ep->mtx during mutex_unlock().

6. Broader Kernel Security Implications
This fix underscores that mutexes provide mutual exclusion, not lifetime guarantees; refcount transitions that may enable object freeing must not occur before or during unlock. Similar patterns elsewhere should be audited to avoid subtle UAFs during unlock paths. Correct separation of lifetime management (atomic refcounts) from mutual exclusion (mutexes/spinlocks) reduces complex race conditions and hard-to-reproduce UAF bugs in kernel subsystems.

[REQUEST]
Produce a detection plan for a CodeQL-based static checker that would detect similar pre-patch flaws.
Requirements for the plan:
- High-level detection goal (1–2 lines).
- A numbered list of detection steps (objective, conceptual AST/semantic signals, FP mitigation).
- Types of program elements to target (functions, call sites, allocation sites, condition checks, lock boundaries, function return-value uses, etc.).
- Dataflow/taint patterns to consider (if applicable), described conceptually.
- Minimal test cases to validate the checker (positive/negative examples).
- Estimated effort/priority (low/medium/high) and likely false-positive sources.
- A short "Limitations & Assumptions" block.

OUTPUT FORMAT (produce exactly this structure; no extra text):
1. Plan Summary
{one-line summary}

2. Detection Steps
1) Step 1: {objective — conceptual signals — FP mitigation}
2) Step 2: {objective — conceptual signals — FP mitigation}
...
(narrow to ≤12 steps)

3. Target Elements
- {list of element types to inspect}

4. Dataflow / Taint Considerations
- {conceptual taint/flow rules to track}

5. Validation & Test Cases
- Positive: {brief}
- Negative: {brief}
- Test harness notes: {brief}

6. Estimated Effort & Priority
{low/medium/high}

7. Likely False-Positive Sources & Mitigations
- {list}

8. Limitations & Assumptions
- {explicit missing info or assumptions}

CONSTRAINTS:
- Do not emit any CodeQL, SQL, pseudocode, or query fragments.
- Keep answers evidence-based and reference which provided field supported each major choice (e.g., “based on [PATCH_DIFF] hunk that adds X”).
- Output must be machine-parseable: keep the exact numbered section headings as above.