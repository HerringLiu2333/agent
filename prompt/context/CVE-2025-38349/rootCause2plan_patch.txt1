You are a senior static-analysis engineer and CodeQL design expert.
Task: Based only on the supplied materials, produce a human-readable detection plan (natural-language) for a CodeQL checker that would detect the same class of vulnerability described. DO NOT produce any query code or pseudocode — the output must be purely natural language steps and rationale.

Rules:
1. Only use evidence contained in the sections labeled [PATCH_DESCRIPTION], [PATCH_DIFF], and [ROOTCAUSE_ANALYSIS]. Do not invent facts.
2. The plan must be a step-by-step detection strategy in natural language (numbered steps). No code, no query snippets, no regex, no domain-specific language.
3. Each step must state: objective, signals to look for (AST/semantic patterns expressed conceptually).
4. List limitations and assumptions (what is Not determinable from inputs). If information required to design an accurate checker is missing, state it explicitly under "Limitations & Assumptions".
5. Keep the plan concise: overall ≤ 12 numbered steps and each step ≤ 2 concise sentences. Use bullets where helpful.
6. Please try to use the simplest way and fewer steps to achieve your goal. But for every step, your response should be as concrete as possible so that I can easily follow your guidance and write a correct checker!

[META]
CVE_NAME: CVE-2025-38349

[INFO]
[PATCH_DESCRIPTION]
    eventpoll: don't decrement ep refcount while still holding the ep mutex
    Jann Horn points out that epoll is decrementing the ep refcount and then
    doing a
    
        mutex_unlock(&ep->mtx);
    
    afterwards. That's very wrong, because it can lead to a use-after-free.
    
    That pattern is actually fine for the very last reference, because the
    code in question will delay the actual call to "ep_free(ep)" until after
    it has unlocked the mutex.
    
    But it's wrong for the much subtler "next to last" case when somebody
    *else* may also be dropping their reference and free the ep while we're
    still using the mutex.
    
    Note that this is true even if that other user is also using the same ep
    mutex: mutexes, unlike spinlocks, can not be used for object ownership,
    even if they guarantee mutual exclusion.
    
    A mutex "unlock" operation is not atomic, and as one user is still
    accessing the mutex as part of unlocking it, another user can come in
    and get the now released mutex and free the data structure while the
    first user is still cleaning up.
    
    See our mutex documentation in Documentation/locking/mutex-design.rst,
    in particular the section [1] about semantics:
    
    	"mutex_unlock() may access the mutex structure even after it has
    	 internally released the lock already - so it's not safe for
    	 another context to acquire the mutex and assume that the
    	 mutex_unlock() context is not using the structure anymore"
    
    So if we drop our ep ref before the mutex unlock, but we weren't the
    last one, we may then unlock the mutex, another user comes in, drops
    _their_ reference and releases the 'ep' as it now has no users - all
    while the mutex_unlock() is still accessing it.
    
    Fix this by simply moving the ep refcount dropping to outside the mutex:
    the refcount itself is atomic, and doesn't need mutex protection (that's
    the whole _point_ of refcounts: unlike mutexes, they are inherently
    about object lifetimes).

[PATCH_DIFF]
    diff --git a/fs/eventpoll.c b/fs/eventpoll.c
    index a97a771a459c9c..895256cd2786e2 100644
    --- a/fs/eventpoll.c
    +++ b/fs/eventpoll.c
    @@ -828,7 +828,7 @@ static bool __ep_remove(struct eventpoll *ep, struct epitem *epi, bool force)
     	kfree_rcu(epi, rcu);
     
     	percpu_counter_dec(&ep->user->epoll_watches);
    -	return ep_refcount_dec_and_test(ep);
    +	return true;
     }
     
     /*
    @@ -836,14 +836,14 @@ static bool __ep_remove(struct eventpoll *ep, struct epitem *epi, bool force)
      */
     static void ep_remove_safe(struct eventpoll *ep, struct epitem *epi)
     {
    -	WARN_ON_ONCE(__ep_remove(ep, epi, false));
    +	if (__ep_remove(ep, epi, false))
    +		WARN_ON_ONCE(ep_refcount_dec_and_test(ep));
     }
     
     static void ep_clear_and_put(struct eventpoll *ep)
     {
     	struct rb_node *rbp, *next;
     	struct epitem *epi;
    -	bool dispose;
     
     	/* We need to release all tasks waiting for these file */
     	if (waitqueue_active(&ep->poll_wait))
    @@ -876,10 +876,8 @@ static void ep_clear_and_put(struct eventpoll *ep)
     		cond_resched();
     	}
     
    -	dispose = ep_refcount_dec_and_test(ep);
     	mutex_unlock(&ep->mtx);
    -
    -	if (dispose)
    +	if (ep_refcount_dec_and_test(ep))
     		ep_free(ep);
     }
     
    @@ -1100,7 +1098,7 @@ again:
     		dispose = __ep_remove(ep, epi, true);
     		mutex_unlock(&ep->mtx);
     
    -		if (dispose)
    +		if (dispose && ep_refcount_dec_and_test(ep))
     			ep_free(ep);
     		goto again;
     	}



[ROOTCAUSE_ANALYSIS]
1. CVE Identifier
CVE-2025-38349

2. Vulnerability Type
Use-after-free due to incorrect refcount/lifetime management and improper lock/unlock ordering

3. Root Cause Summary
The epoll code decremented the struct eventpoll reference count while holding ep->mtx and then executed mutex_unlock(&ep->mtx). If the decrement was the “next-to-last” reference, a concurrent user could acquire the now-unlocked mutex, drop the last reference, and free the eventpoll while the first thread’s mutex_unlock still accesses the mutex structure, causing a use-after-free. The pre-patch logic incorrectly relied on a mutex for object lifetime guarantees, which mutexes do not provide because mutex_unlock is not atomic wrt structure access. The patch moves the refcount decrement outside the mutex critical section, ensuring the object remains alive until after unlock completes.

4. Kernel Subsystem Analysis
1) Affected Subsystem:
fs/eventpoll (epoll)

2) Pre-Patch Flaw:
- __ep_remove(ep, epi, ...) returned ep_refcount_dec_and_test(ep), performing the decrement under ep->mtx.
- ep_clear_and_put() computed dispose = ep_refcount_dec_and_test(ep) before mutex_unlock(&ep->mtx), then freed ep if dispose was true.
- Similar pattern around “again:” path: dispose = __ep_remove(...); mutex_unlock(&ep->mtx); if (dispose) ep_free(ep).
- This ordered “decrement refcount” -> “unlock mutex” allowed other threads to free ep between the decrement and the completion of mutex_unlock.

3) Trigger Condition:
Concurrent reference drops on the same epoll instance where one thread performs the “next-to-last” refcount decrement inside the mutex and then unlocks, while another thread performs the last refcount decrement and frees ep after acquiring the mutex.

4) Impact Mechanism:
During mutex_unlock(&ep->mtx), the unlocking thread may still access the mutex structure associated with ep; if ep has been freed by another thread, this results in use-after-free, leading to potential memory corruption or kernel crashes.

5. Patch Analysis
1) Fix Approach:
Move all ep_refcount_dec_and_test(ep) calls out of the ep->mtx critical section and after mutex_unlock, relying on atomic refcounts for lifetime management instead of the mutex.

2) Key Code Changes:
- __ep_remove(): changed return from ep_refcount_dec_and_test(ep) to return true, eliminating the in-mutex refcount decrement.
- ep_remove_safe(): now calls __ep_remove(ep, epi, false) and, if it returns true, performs WARN_ON_ONCE(ep_refcount_dec_and_test(ep)) outside the removal logic.
- ep_clear_and_put(): removed local “dispose”; performs mutex_unlock(&ep->mtx) first, then calls ep_refcount_dec_and_test(ep) and ep_free(ep) if it was the last reference.
- “again:” path (~line 1100): after __ep_remove() and mutex_unlock, changed conditional to if (dispose && ep_refcount_dec_and_test(ep)) ep_free(ep), ensuring decrement occurs post-unlock.

3) Locking/Concurrency Impact:
- Eliminates the unsafe window where a refcount drop inside the mutex could enable another thread to free the object while the first thread is still unlocking.
- Ensures object lifetime extends through the entire mutex_unlock operation; final freeing occurs only after unlock completes.
- Uses atomic refcount semantics for lifetime, decoupling object ownership from mutex mutual exclusion, in line with Documentation/locking/mutex-design.rst.

6. Broader Kernel Security Implications
This fix underscores that mutexes cannot be used to guarantee object lifetime and that refcount operations must be carefully ordered relative to lock/unlock to avoid UAF races. Similar patterns elsewhere in the kernel should be audited for refcount drops inside locks followed by mutex_unlock. Correctly separating lifetime management (atomic refcounts, RCU) from mutual exclusion reduces subtle race-induced memory safety bugs and improves robustness against exploitation.

[REQUEST]
Produce a detection plan for a CodeQL-based static checker that would detect similar pre-patch flaws.
Requirements for the plan:
- High-level detection goal (1–2 lines).
- A numbered list of detection steps (objective, conceptual AST/semantic signals).
- A short "Limitations & Assumptions" block.

OUTPUT FORMAT (produce exactly this structure; no extra text):
1. Plan Summary
{one-line summary}

2. Detection Steps
1) Step 1: {objective — conceptual signals}
2) Step 2: {objective — conceptual signals}
...
(narrow to ≤12 steps)

3. Limitations & Assumptions
- {explicit missing info or assumptions}